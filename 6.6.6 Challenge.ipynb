{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6.6 Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['6.6.6 Challenge.ipynb', '.ipynb_checkpoints', 'fruits-360_dataset_2018_02_08']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now take your Keras skills and go build another neural network. Pick your data set, but it should be one of abstract types, possibly even nonnumeric, and use Keras to make five implementations of your network. Compare them both in computational complexity as well as in accuracy and given that tradeoff decide which one you like best.\n",
    "\n",
    "Your dataset should be sufficiently large for a neural network to perform well (samples should really be in the thousands here) and try to pick something that takes advantage of neural networks’ ability to have both feature extraction and supervised capabilities, so don’t pick something with an easy to consume list of features already generated for you (though neural networks can still be useful in those contexts).\n",
    "\n",
    "Note that if you want to use an unprocessed image dataset, scikit-image is a useful package for converting to importable numerics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "The goal of this project is to implement the Keras skills, and use them to build five different models on a given dataset, evaluate them and choose the one we like best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "I chose a dataset with a selection 75 different fruits, containing 50590 images. The goal is to split the data into training and test sets, train the corresponding set with each of our models, and evaluate it's performance on the testing set, on how well does it identify each fruit.\n",
    "\n",
    "__Dataset properties:__\n",
    "\n",
    "Total number of images: 50590.\n",
    "\n",
    "Training set size: 37836 images (one fruit per image).\n",
    "\n",
    "Test set size: 12709 images (one fruit per image).\n",
    "\n",
    "Multi-fruits set size: 45 images (more than one fruit (or fruit class) per image)\n",
    "\n",
    "Number of classes: 75 (fruits).\n",
    "\n",
    "Image size: 100x100 pixels.\n",
    "\n",
    "Filename format: image_index_100.jpg (e.g. 32_100.jpg) or r_image_index_100.jpg (e.g. r_32_100.jpg) or r2_image_index_100.jpg. \"r\" stands for rotated fruit. \"r2\" means that the fruit was rotated around the 3rd axis. \"100\" comes from image size (100x100 pixels).\n",
    "\n",
    "Different varieties of the same fruit (apple for instance) are stored as belonging to different classes.\n",
    "\n",
    "__Note:__\n",
    "\n",
    "I used aproximately half of the dataset since my computer freezed each time I tried to train and test with all the original number of images.\n",
    "\n",
    "__Sections:__\n",
    "\n",
    "[Text Processing](#section1)<br>\n",
    "[Unsupervised Feature Generation](#section2)<br>\n",
    "[Supervised Learning Models](#section3)<br>\n",
    "[Conclusion](#section4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training set\n",
    "\n",
    "The dataset is already divided into training and testing folders from it's source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Listing the training set\n",
    "fruit_images = []\n",
    "labels = [] \n",
    "for fruit_dir_path in glob.glob(\"../input/*/fruits-360/Training/*\"):\n",
    "    fruit_label = fruit_dir_path.split(\"/\")[-1]\n",
    "    for image_path in glob.glob(os.path.join(fruit_dir_path, \"*.jpg\")):\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "        \n",
    "        image = cv2.resize(image, (45, 45))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        fruit_images.append(image)\n",
    "        labels.append(fruit_label)\n",
    "fruit_images = np.array(fruit_images)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_to_id_dict = {v:i for i,v in enumerate(np.unique(labels))}\n",
    "id_to_label_dict = {v: k for k, v in label_to_id_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Apple Braeburn',\n",
       " 1: 'Apple Golden 1',\n",
       " 2: 'Apple Golden 2',\n",
       " 3: 'Apple Golden 3',\n",
       " 4: 'Apple Granny Smith',\n",
       " 5: 'Apple Red 1',\n",
       " 6: 'Apple Red 2',\n",
       " 7: 'Apple Red 3',\n",
       " 8: 'Apple Red Delicious',\n",
       " 9: 'Apple Red Yellow',\n",
       " 10: 'Apricot',\n",
       " 11: 'Avocado',\n",
       " 12: 'Avocado ripe',\n",
       " 13: 'Banana',\n",
       " 14: 'Banana Red',\n",
       " 15: 'Cactus fruit',\n",
       " 16: 'Cantaloupe 1',\n",
       " 17: 'Cantaloupe 2',\n",
       " 18: 'Carambula',\n",
       " 19: 'Cherry 1',\n",
       " 20: 'Cherry 2',\n",
       " 21: 'Cherry Rainier',\n",
       " 22: 'Cherry Wax Black',\n",
       " 23: 'Cherry Wax Red',\n",
       " 24: 'Cherry Wax Yellow',\n",
       " 25: 'Clementine',\n",
       " 26: 'Cocos'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_ids = np.array([label_to_id_dict[x] for x in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13622, 45, 45, 3), (13622,), (13622,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fruit_images.shape, label_ids.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Listing the testing set\n",
    "validation_fruit_images = []\n",
    "validation_labels = [] \n",
    "for fruit_dir_path in glob.glob(\"../input/*/fruits-360/Test/*\"):\n",
    "    fruit_label = fruit_dir_path.split(\"/\")[-1]\n",
    "    for image_path in glob.glob(os.path.join(fruit_dir_path, \"*.jpg\")):\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "        \n",
    "        image = cv2.resize(image, (45, 45))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        validation_fruit_images.append(image)\n",
    "        validation_labels.append(fruit_label)\n",
    "validation_fruit_images = np.array(validation_fruit_images)\n",
    "validation_labels = np.array(validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_label_ids = np.array([label_to_id_dict[x] for x in validation_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4564, 45, 45, 3), (4564,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_fruit_images.shape, validation_label_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sizes: (13622, 45, 45, 3) (4564, 45, 45, 3) (13622, 60) (4564, 60)\n",
      "Flattened: (13622, 6075) (4564, 6075)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = fruit_images, validation_fruit_images\n",
    "Y_train, Y_test = label_ids, validation_label_ids\n",
    "\n",
    "#Normalize color values to between 0 and 1\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255\n",
    "\n",
    "#Reshaping to a flattened version\n",
    "X_flat_train = X_train.reshape(X_train.shape[0], 45*45*3)\n",
    "X_flat_test = X_test.reshape(X_test.shape[0], 45*45*3)\n",
    "\n",
    "#Encoding the output\n",
    "Y_train = keras.utils.to_categorical(Y_train, 60)\n",
    "Y_test = keras.utils.to_categorical(Y_test, 60)\n",
    "\n",
    "print('Original Sizes:', X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "print('Flattened:', X_flat_train.shape, X_flat_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 45, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa85b6b86d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(X_train[0].shape)\n",
    "plt.imshow(X_train[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers import LSTM, Input, TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "\n",
    "# Import the backend\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Multi Layer Perceptron\n",
    "We will start with a simple sequential model using two dense layers of 128 nodes and 64 nodes respectively with a Rectified Linear Unit activation funcion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               777728    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 60)                3900      \n",
      "=================================================================\n",
      "Total params: 789,884\n",
      "Trainable params: 789,884\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 13622 samples, validate on 4564 samples\n",
      "Epoch 1/10\n",
      "13622/13622 [==============================] - 6s 429us/step - loss: 2.9407 - acc: 0.2285 - val_loss: 1.5884 - val_acc: 0.5480\n",
      "Epoch 2/10\n",
      "13622/13622 [==============================] - 5s 332us/step - loss: 1.2730 - acc: 0.5645 - val_loss: 0.8567 - val_acc: 0.7046\n",
      "Epoch 3/10\n",
      "13622/13622 [==============================] - 5s 336us/step - loss: 0.8338 - acc: 0.7002 - val_loss: 0.6980 - val_acc: 0.7732\n",
      "Epoch 4/10\n",
      "13622/13622 [==============================] - 5s 334us/step - loss: 0.6231 - acc: 0.7737 - val_loss: 0.6085 - val_acc: 0.7881\n",
      "Epoch 5/10\n",
      "13622/13622 [==============================] - 5s 335us/step - loss: 0.4862 - acc: 0.8196 - val_loss: 0.6966 - val_acc: 0.7531\n",
      "Epoch 6/10\n",
      "13622/13622 [==============================] - 5s 342us/step - loss: 0.4074 - acc: 0.8512 - val_loss: 0.6568 - val_acc: 0.7851\n",
      "Epoch 7/10\n",
      "13622/13622 [==============================] - 5s 341us/step - loss: 0.3489 - acc: 0.8710 - val_loss: 0.6239 - val_acc: 0.7877\n",
      "Epoch 8/10\n",
      "13622/13622 [==============================] - 5s 336us/step - loss: 0.3082 - acc: 0.8856 - val_loss: 0.4182 - val_acc: 0.8620\n",
      "Epoch 9/10\n",
      "13622/13622 [==============================] - 5s 335us/step - loss: 0.2710 - acc: 0.8994 - val_loss: 0.3995 - val_acc: 0.8749\n",
      "Epoch 10/10\n",
      "13622/13622 [==============================] - 5s 336us/step - loss: 0.2675 - acc: 0.9010 - val_loss: 0.5242 - val_acc: 0.8429\n",
      "Test loss: 0.524219473228\n",
      "Test accuracy: 0.842900964119\n"
     ]
    }
   ],
   "source": [
    "model_dense = Sequential()\n",
    "\n",
    "# Add dense layers to create a fully connected MLP\n",
    "# Note that we specify an input shape for the first layer, but only the first layer.\n",
    "# Relu is the activation function used\n",
    "model_dense.add(Dense(128, activation='relu', input_shape=(X_flat_train.shape[1],)))\n",
    "# Dropout layers remove features and fight overfitting\n",
    "model_dense.add(Dropout(0.1))\n",
    "model_dense.add(Dense(64, activation='relu'))\n",
    "model_dense.add(Dropout(0.1))\n",
    "# End with a number of units equal to the number of classes we have for our outcome\n",
    "model_dense.add(Dense(60, activation='softmax'))\n",
    "\n",
    "model_dense.summary()\n",
    "\n",
    "# Compile the model to put it all together.\n",
    "model_dense.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history_dense = model_dense.fit(X_flat_train, Y_train,\n",
    "                          batch_size=128,\n",
    "                          epochs=10,\n",
    "                          verbose=1,\n",
    "                          validation_data=(X_flat_test, Y_test))\n",
    "score = model_dense.evaluate(X_flat_test, Y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Multi Layer Perceptron\n",
    "We will now modify our latter model and add four dense layers, being a total of five, having the following nodes: 256, 128, 128, 128, 128, for each layer respectively, with a Rectified Linear Unit activation funcion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 256)               1555456   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 60)                7740      \n",
      "=================================================================\n",
      "Total params: 1,645,628\n",
      "Trainable params: 1,645,628\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 13622 samples, validate on 4564 samples\n",
      "Epoch 1/10\n",
      "13622/13622 [==============================] - 10s 758us/step - loss: 2.4174 - acc: 0.2756 - val_loss: 1.3288 - val_acc: 0.5121\n",
      "Epoch 2/10\n",
      "13622/13622 [==============================] - 11s 775us/step - loss: 1.0558 - acc: 0.6267 - val_loss: 0.8063 - val_acc: 0.7436\n",
      "Epoch 3/10\n",
      "13622/13622 [==============================] - 10s 755us/step - loss: 0.7086 - acc: 0.7500 - val_loss: 0.5296 - val_acc: 0.8295\n",
      "Epoch 4/10\n",
      "13622/13622 [==============================] - 9s 689us/step - loss: 0.5579 - acc: 0.8061 - val_loss: 0.5363 - val_acc: 0.7566\n",
      "Epoch 5/10\n",
      "13622/13622 [==============================] - 10s 739us/step - loss: 0.4432 - acc: 0.8502 - val_loss: 1.0701 - val_acc: 0.6733\n",
      "Epoch 6/10\n",
      "13622/13622 [==============================] - 12s 903us/step - loss: 0.3358 - acc: 0.8846 - val_loss: 0.7649 - val_acc: 0.8278\n",
      "Epoch 7/10\n",
      "13622/13622 [==============================] - 10s 756us/step - loss: 0.3388 - acc: 0.8927 - val_loss: 0.2803 - val_acc: 0.9071\n",
      "Epoch 8/10\n",
      "13622/13622 [==============================] - 10s 753us/step - loss: 0.2901 - acc: 0.9074 - val_loss: 0.3371 - val_acc: 0.8918\n",
      "Epoch 9/10\n",
      "13622/13622 [==============================] - 9s 687us/step - loss: 0.2309 - acc: 0.9227 - val_loss: 0.2997 - val_acc: 0.8933\n",
      "Epoch 10/10\n",
      "13622/13622 [==============================] - 10s 769us/step - loss: 0.2284 - acc: 0.9290 - val_loss: 0.7757 - val_acc: 0.7691\n",
      "Test loss: 0.775687709218\n",
      "Test accuracy: 0.769062226144\n"
     ]
    }
   ],
   "source": [
    "model_deep = Sequential()\n",
    "\n",
    "# Add dense layers to create a fully connected MLP\n",
    "# Note that we specify an input shape for the first layer, but only the first layer.\n",
    "# Relu is the activation function used\n",
    "model_deep.add(Dense(256, activation='relu', input_shape=(X_flat_train.shape[1],)))\n",
    "# Dropout layers remove features and fight overfitting\n",
    "model_deep.add(Dropout(0.05))\n",
    "model_deep.add(Dense(128, activation='relu'))\n",
    "model_deep.add(Dropout(0.05))\n",
    "model_deep.add(Dense(128, activation='relu'))\n",
    "model_deep.add(Dropout(0.05))\n",
    "model_deep.add(Dense(128, activation='relu'))\n",
    "model_deep.add(Dropout(0.05))\n",
    "model_deep.add(Dense(128, activation='relu'))\n",
    "model_deep.add(Dropout(0.05))\n",
    "# End with a number of units equal to the number of classes we have for our outcome\n",
    "model_deep.add(Dense(60, activation='softmax'))\n",
    "\n",
    "model_deep.summary()\n",
    "\n",
    "# Compile the model to put it all together.\n",
    "model_deep.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history_deep = model_deep.fit(X_flat_train, Y_train,\n",
    "                          batch_size=128,\n",
    "                          epochs=10,\n",
    "                          verbose=1,\n",
    "                          validation_data=(X_flat_test, Y_test))\n",
    "score = model_deep.evaluate(X_flat_test, Y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Convolutional Neural Network\n",
    "\n",
    "Now will build and test Convolutional Neural Network, consisting of four layers with 32, 64, 128 and 60 nodes. We will run this model at one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13622 samples, validate on 4564 samples\n",
      "Epoch 1/1\n",
      "13622/13622 [==============================] - 250s 18ms/step - loss: 1.9728 - acc: 0.4316 - val_loss: 0.7558 - val_acc: 0.7248\n",
      "Test loss: 0.755786098638\n",
      "Test accuracy: 0.72480280461\n"
     ]
    }
   ],
   "source": [
    "model_cnn = Sequential()\n",
    "# First convolutional layer, note the specification of shape\n",
    "model_cnn.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(45, 45, 3)))\n",
    "model_cnn.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_cnn.add(Dropout(0.25))\n",
    "model_cnn.add(Flatten())\n",
    "model_cnn.add(Dense(128, activation='relu'))\n",
    "model_cnn.add(Dropout(0.5))\n",
    "model_cnn.add(Dense(60, activation='softmax'))\n",
    "\n",
    "model_cnn.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_cnn.fit(X_train, Y_train,\n",
    "          batch_size=128,\n",
    "          epochs=1,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test))\n",
    "score = model_cnn.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Convolutional Neural Network\n",
    "\n",
    "Now will use the same latter model, but we will train it for 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13622 samples, validate on 4564 samples\n",
      "Epoch 1/10\n",
      "13622/13622 [==============================] - 242s 18ms/step - loss: 0.5352 - acc: 0.8137 - val_loss: 0.3218 - val_acc: 0.8946\n",
      "Epoch 2/10\n",
      "13622/13622 [==============================] - 243s 18ms/step - loss: 0.2384 - acc: 0.9137 - val_loss: 0.2341 - val_acc: 0.9016\n",
      "Epoch 3/10\n",
      "13622/13622 [==============================] - 244s 18ms/step - loss: 0.1376 - acc: 0.9496 - val_loss: 0.2062 - val_acc: 0.9360\n",
      "Epoch 4/10\n",
      "13622/13622 [==============================] - 243s 18ms/step - loss: 0.1144 - acc: 0.9567 - val_loss: 0.2048 - val_acc: 0.9373\n",
      "Epoch 5/10\n",
      "13622/13622 [==============================] - 244s 18ms/step - loss: 0.0774 - acc: 0.9690 - val_loss: 0.1545 - val_acc: 0.9340\n",
      "Epoch 6/10\n",
      "13622/13622 [==============================] - 243s 18ms/step - loss: 0.0756 - acc: 0.9685 - val_loss: 0.1348 - val_acc: 0.9437\n",
      "Epoch 7/10\n",
      "13622/13622 [==============================] - 243s 18ms/step - loss: 0.0798 - acc: 0.9658 - val_loss: 0.1508 - val_acc: 0.9384\n",
      "Epoch 8/10\n",
      "13622/13622 [==============================] - 243s 18ms/step - loss: 0.0590 - acc: 0.9710 - val_loss: 0.8178 - val_acc: 0.8284\n",
      "Epoch 9/10\n",
      "13622/13622 [==============================] - 243s 18ms/step - loss: 0.0643 - acc: 0.9711 - val_loss: 0.1335 - val_acc: 0.9540\n",
      "Epoch 10/10\n",
      "13622/13622 [==============================] - 243s 18ms/step - loss: 0.0508 - acc: 0.9750 - val_loss: 0.1375 - val_acc: 0.9408\n",
      "Test loss: 0.137483024168\n",
      "Test accuracy: 0.940841367274\n"
     ]
    }
   ],
   "source": [
    "model_cnn.fit(X_train, Y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test))\n",
    "score = model_cnn.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Hierarchical Recurrrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = fruit_images\n",
    "x_test = validation_fruit_images\n",
    "y_train = label_ids\n",
    "y_test = validation_label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (13622, 45, 45, 3)\n",
      "13622 train samples\n",
      "4564 test samples\n",
      "Train on 13622 samples, validate on 4564 samples\n",
      "Epoch 1/3\n",
      "13622/13622 [==============================] - 133s 10ms/step - loss: 3.0679 - acc: 0.0926 - val_loss: 2.6175 - val_acc: 0.2077\n",
      "Epoch 2/3\n",
      "13622/13622 [==============================] - 133s 10ms/step - loss: 2.3357 - acc: 0.2526 - val_loss: 2.2750 - val_acc: 0.2734\n",
      "Epoch 3/3\n",
      "13622/13622 [==============================] - 132s 10ms/step - loss: 1.8882 - acc: 0.3668 - val_loss: 1.6702 - val_acc: 0.4124\n",
      "Test loss: 1.67024440811\n",
      "Test accuracy: 0.412357581017\n"
     ]
    }
   ],
   "source": [
    "# Training parameters.\n",
    "batch_size = 64\n",
    "num_classes = 27\n",
    "epochs = 3\n",
    "\n",
    "# Embedding dimensions.\n",
    "row_hidden = 32\n",
    "col_hidden = 32\n",
    "\n",
    "# Reshapes data to 4D for Hierarchical RNN.\n",
    "x_train = x_train.reshape(x_train.shape[0], 45, 45, 3)\n",
    "x_test = x_test.reshape(x_test.shape[0], 45, 45, 3)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Converts class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "row, col, pixel = x_train.shape[1:]\n",
    "\n",
    "# 4D input.\n",
    "x = Input(shape=(row, col, pixel))\n",
    "\n",
    "# Encodes a row of pixels using TimeDistributed Wrapper.\n",
    "encoded_rows = TimeDistributed(LSTM(row_hidden))(x)\n",
    "\n",
    "# Encodes columns of encoded rows.\n",
    "encoded_columns = LSTM(col_hidden)(encoded_rows)\n",
    "\n",
    "# Final predictions and model.\n",
    "prediction = Dense(num_classes, activation='softmax')(encoded_columns)\n",
    "model = Model(x, prediction)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Training.\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluation.\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conlcusion\n",
    "\n",
    "After running five different models, I can say that our first model MLP  was the most efficient given it's low running time and high accuracy. We can of course tune even more the different parameters, and try to optimize our model yielding a higher result. I was however surprised with the results of our last model using LSTM, however I believe we can raise this result modifying it's layers and adding more data to our dataset.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
